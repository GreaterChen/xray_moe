import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import pickle
import os
from datasets import MIMIC
from collections import defaultdict


class AnatomicalTextEnhancer(nn.Module):
    """
    è§£å‰–åŒºåŸŸæ–‡æœ¬å¢å¼ºæ¨¡å— - åŸºäºè§†è§‰ç‰¹å¾æ£€ç´¢
    é€šè¿‡è§†è§‰ç‰¹å¾æ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ç‰¹å¾ï¼Œç„¶åè·å–å¯¹åº”çš„æ–‡æœ¬æè¿°
    æ”¯æŒæŒ‰è§£å‰–åŒºåŸŸåˆ†ç»„æ£€ç´¢å’Œæ’é™¤è‡ªèº«æ ·æœ¬
    """
    
    def __init__(self, tokenizer, visual_projection=None, text_projection=None, device="cuda", config=None):
        super(AnatomicalTextEnhancer, self).__init__()
        
        self.tokenizer = tokenizer
        self.device = device
        self.visual_projection = visual_projection
        self.text_projection = text_projection
        
        # ä»é…ç½®ä¸­è·å–å‚æ•°
        if config is not None:
            # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ–‡æœ¬å¢å¼º
            self.enabled = getattr(config, 'ENABLE_TEXT_ENHANCEMENT', False)
            
            # å¦‚æœæœªå¯ç”¨ï¼Œç›´æ¥è¿”å›
            if not self.enabled:
                print("æ–‡æœ¬å¢å¼ºåŠŸèƒ½å·²è¢«é…ç½®ç¦ç”¨")
                return
            
            # è·å–æ•°æ®åº“è·¯å¾„
            self.knowledge_base_path = getattr(config, 'TEXT_ENHANCEMENT_DB_PATH', 
                                             "/mnt/chenlb/datasets/MIMIC/visual_text_knowledge_base.pkl")
        else:
            # é»˜è®¤é…ç½®ï¼ˆå‘åå…¼å®¹ï¼‰
            print("è­¦å‘Š: æœªæä¾›é…ç½®ï¼Œä½¿ç”¨é»˜è®¤è®¾ç½®å¯ç”¨æ–‡æœ¬å¢å¼º")
            self.enabled = True
            self.knowledge_base_path = "/mnt/chenlb/datasets/MIMIC/visual_text_knowledge_base.pkl"
        
        # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.knowledge_base_path):
            print(f"è­¦å‘Š: çŸ¥è¯†åº“æ–‡ä»¶ {self.knowledge_base_path} ä¸å­˜åœ¨ï¼Œæ–‡æœ¬å¢å¼ºåŠŸèƒ½å°†è¢«ç¦ç”¨")
            self.enabled = False
            return
        
        try:
            # åŠ è½½çŸ¥è¯†åº“
            print(f"æ­£åœ¨åŠ è½½æ–‡æœ¬å¢å¼ºæ•°æ®åº“: {self.knowledge_base_path}")
            with open(self.knowledge_base_path, 'rb') as f:
                kb_data = pickle.load(f)
            
            self.knowledge_base = kb_data['knowledge_base']
            self.feature_to_info_map = kb_data['feature_to_info_map']
            self.statistics = kb_data['statistics']
            
            # è·å–è§£å‰–åŒºåŸŸåˆ—è¡¨ï¼ˆä¸MIMIC.ANATOMICAL_REGIONSå¯¹åº”ï¼‰
            anatomical_regions = MIMIC.ANATOMICAL_REGIONS
            region_name_to_idx = {name: idx for idx, name in enumerate(anatomical_regions)}
            
            # ç›´æ¥æŒ‰è§£å‰–åŒºåŸŸç»„ç»‡çŸ¥è¯†åº“ï¼Œé¿å…é‡å¤å­˜å‚¨
            self.region_features_db = {}  # æŒ‰åŒºåŸŸå­˜å‚¨çš„ç‰¹å¾å¼ é‡
            self.region_indices = {}      # æŒ‰åŒºåŸŸå­˜å‚¨çš„åŸå§‹ç´¢å¼•
            self.region_image_ids = {}    # æŒ‰åŒºåŸŸå­˜å‚¨çš„image_id
            self.region_texts = {}        # æŒ‰åŒºåŸŸå­˜å‚¨çš„æ–‡æœ¬æè¿°
            
            # é¢„åˆ†é…å­˜å‚¨ç»“æ„
            region_data_temp = {idx: {'features': [], 'indices': [], 'image_ids': [], 'texts': []} 
                               for idx in range(len(anatomical_regions))}
            
            # ä¸€æ¬¡éå†ç»„ç»‡æ‰€æœ‰æ•°æ®
            for idx, entry in enumerate(self.knowledge_base):
                region_name = entry['region_name']
                if region_name in region_name_to_idx:
                    region_idx = region_name_to_idx[region_name]
                    region_data_temp[region_idx]['features'].append(entry['visual_feature'])
                    region_data_temp[region_idx]['indices'].append(idx)
                    region_data_temp[region_idx]['image_ids'].append(entry['image_id'])
                    region_data_temp[region_idx]['texts'].append(entry['text_string'])
            
            # è½¬æ¢ä¸ºtensorå¹¶å½’ä¸€åŒ–ï¼Œåªå¤„ç†æœ‰æ•°æ®çš„åŒºåŸŸ
            successful_regions = 0
            for region_idx, region_name in enumerate(anatomical_regions):
                region_data = region_data_temp[region_idx]
                
                if region_data['features']:  # ç¡®ä¿ä¸ä¸ºç©º
                    # æ‰¹é‡è½¬æ¢å’Œå½’ä¸€åŒ–
                    features_array = np.stack(region_data['features'])
                    region_features_tensor = torch.tensor(
                        features_array, 
                        dtype=torch.float32, 
                        device=device
                    )
                    region_features_tensor = F.normalize(region_features_tensor, p=2, dim=1)
                    
                    self.region_features_db[region_idx] = region_features_tensor
                    self.region_indices[region_idx] = region_data['indices']
                    self.region_image_ids[region_idx] = region_data['image_ids']
                    self.region_texts[region_idx] = region_data['texts']
                    successful_regions += 1
                else:
                    print(f"è­¦å‘Š: è§£å‰–åŒºåŸŸ '{region_name}' (ç´¢å¼•{region_idx}) æ²¡æœ‰æ•°æ®")
            
            # æ¸…ç†ä¸´æ—¶æ•°æ®
            del region_data_temp
            del self.knowledge_base  # é‡Šæ”¾åŸå§‹çŸ¥è¯†åº“å†…å­˜
            
            print(f"âœ… æ–‡æœ¬å¢å¼ºæ•°æ®åº“åŠ è½½æˆåŠŸ:")
            print(f"  - æ•°æ®åº“è·¯å¾„: {self.knowledge_base_path}")
            print(f"  - æ€»æ¡ç›®æ•°: {self.statistics['total_entries']}")
            print(f"  - è¦†ç›–å›¾åƒæ•°: {self.statistics['unique_images']}")
            print(f"  - è§£å‰–åŒºåŸŸæ•°: {self.statistics['unique_regions']}")
            print(f"  - æˆåŠŸç»„ç»‡çš„åŒºåŸŸæ•°: {successful_regions}")
            
            # æ‰“å°æ¯ä¸ªåŒºåŸŸçš„æ•°æ®é‡ï¼ˆå¯é€‰ï¼Œé€šè¿‡é…ç½®æ§åˆ¶ï¼‰
            if hasattr(config, 'SHOW_REGION_STATS') and config.SHOW_REGION_STATS:
                for region_idx, region_name in enumerate(anatomical_regions):
                    if region_idx in self.region_features_db:
                        count = len(self.region_texts[region_idx])
                        print(f"    - {region_name}: {count} ä¸ªæ ·æœ¬")
            
        except Exception as e:
            print(f"âŒ åŠ è½½çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}")
            self.enabled = False
            return
    
    def retrieve_similar_visual_features(self, query_visual_features, query_image_ids=None, top_k=1):
        """
        åŸºäºè§†è§‰ç‰¹å¾æ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ç‰¹å¾åŠå…¶å¯¹åº”çš„æ–‡æœ¬æè¿° - GPUä¼˜åŒ–ç‰ˆæœ¬
        æ”¯æŒæŒ‰è§£å‰–åŒºåŸŸåˆ†ç»„æ£€ç´¢å’Œæ’é™¤è‡ªèº«æ ·æœ¬
        
        Args:
            query_visual_features: [batch_size, num_regions, hidden_size] æŸ¥è¯¢çš„è§†è§‰ç‰¹å¾
            query_image_ids: list of strï¼ŒæŸ¥è¯¢æ ·æœ¬çš„image_idåˆ—è¡¨ï¼Œç”¨äºæ’é™¤è‡ªèº«
            top_k: è¿”å›æœ€ç›¸ä¼¼çš„top-kä¸ªç»“æœ
        
        Returns:
            retrieved_texts: list of listsï¼Œæ¯ä¸ªæ ·æœ¬æ¯ä¸ªåŒºåŸŸçš„æ£€ç´¢ç»“æœæ–‡æœ¬
            similarity_scores: [batch_size, num_regions] æœ€é«˜ç›¸ä¼¼åº¦åˆ†æ•°
        """
        if not self.enabled:
            return None, None
        
        batch_size, num_regions, hidden_size = query_visual_features.shape
        
        # å¦‚æœæ²¡æœ‰æä¾›image_idsï¼Œåˆ›å»ºç©ºåˆ—è¡¨
        if query_image_ids is None:
            query_image_ids = [None] * batch_size
        
        # å½’ä¸€åŒ–æŸ¥è¯¢ç‰¹å¾ - ä¸€æ¬¡æ€§æ“ä½œ
        query_features_norm = F.normalize(query_visual_features, p=2, dim=2)  # [B, 29, 768]
        
        # é¢„åˆ†é…ç»“æœå¼ é‡
        all_similarities = torch.zeros((batch_size, num_regions), device=self.device)
        
        # ä¼˜åŒ–ï¼šé¢„æ„å»ºimage_idåˆ°ç´¢å¼•çš„æ˜ å°„ï¼Œé¿å…é‡å¤æŸ¥æ‰¾
        image_id_maps = {}
        for region_idx in range(num_regions):
            if region_idx in self.region_features_db:
                region_image_ids = self.region_image_ids[region_idx]
                image_id_maps[region_idx] = {img_id: i for i, img_id in enumerate(region_image_ids)}
        
        # æ‰¹é‡å¤„ç†ï¼šå°†æ‰€æœ‰åŒºåŸŸçš„ç‰¹å¾æ‹¼æ¥ï¼Œè¿›è¡Œä¸€æ¬¡å¤§è§„æ¨¡çŸ©é˜µä¹˜æ³•
        if hasattr(self, '_combined_features_cache'):
            # ä½¿ç”¨ç¼“å­˜çš„ç»„åˆç‰¹å¾
            combined_features = self._combined_features_cache
            region_offsets = self._region_offsets_cache
            region_sizes = self._region_sizes_cache
        else:
            # é¦–æ¬¡è®¡ç®—æ—¶æ„å»ºç¼“å­˜
            all_region_features = []
            region_offsets = {}
            region_sizes = {}
            current_offset = 0
            
            for region_idx in range(num_regions):
                if region_idx in self.region_features_db:
                    region_features = self.region_features_db[region_idx]
                    all_region_features.append(region_features)
                    region_offsets[region_idx] = current_offset
                    region_sizes[region_idx] = region_features.size(0)
                    current_offset += region_features.size(0)
                else:
                    region_sizes[region_idx] = 0
            
            if all_region_features:
                combined_features = torch.cat(all_region_features, dim=0)  # [Total_N, 768]
                
                # ç¼“å­˜ç»“æœä»¥å¤‡åç»­ä½¿ç”¨
                self._combined_features_cache = combined_features
                self._region_offsets_cache = region_offsets
                self._region_sizes_cache = region_sizes
            else:
                combined_features = torch.empty((0, hidden_size), device=self.device)
                region_offsets = {}
                region_sizes = {}
        
        # å¦‚æœæ²¡æœ‰ç‰¹å¾æ•°æ®ï¼Œè¿”å›ç©ºç»“æœ
        if combined_features.size(0) == 0:
            retrieved_texts = [[""] * num_regions for _ in range(batch_size)]
            return retrieved_texts, all_similarities
        
        # æ‰¹é‡æ–‡æœ¬ç»“æœé¢„åˆ†é…
        retrieved_texts = []
        
        # æŒ‰batchå¤„ç†ï¼Œä½†ä½¿ç”¨é«˜æ•ˆçš„å¼ é‡æ“ä½œ
        for batch_idx in range(batch_size):
            batch_texts = []
            query_image_id = query_image_ids[batch_idx] 
            
            # æå–å½“å‰batchçš„æŸ¥è¯¢ç‰¹å¾ [29, 768]
            batch_query_features = query_features_norm[batch_idx]  
            
            for region_idx in range(num_regions):
                if region_idx not in self.region_features_db or region_sizes[region_idx] == 0:
                    batch_texts.append("")
                    continue
                
                # è·å–åŒºåŸŸç›¸å…³æ•°æ®
                region_offset = region_offsets[region_idx]
                region_size = region_sizes[region_idx] 
                region_texts = self.region_texts[region_idx]
                
                # æå–å½“å‰åŒºåŸŸçš„æŸ¥è¯¢ç‰¹å¾å’Œæ•°æ®åº“ç‰¹å¾
                query_feat = batch_query_features[region_idx:region_idx+1]  # [1, 768]
                region_features = combined_features[region_offset:region_offset+region_size]  # [N_region, 768]
                
                # è®¡ç®—ç›¸ä¼¼åº¦
                similarities = torch.mm(query_feat, region_features.t()).squeeze(0)  # [N_region]
                
                # å¤„ç†æ’é™¤è‡ªèº«çš„é€»è¾‘
                if query_image_id is not None and region_idx in image_id_maps:
                    image_id_map = image_id_maps[region_idx]
                    if query_image_id in image_id_map:
                        exclude_idx = image_id_map[query_image_id]
                        similarities[exclude_idx] = -float('inf')
                
                # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå€™é€‰
                valid_similarities = similarities[similarities != -float('inf')]
                if len(valid_similarities) == 0:
                    batch_texts.append("")
                    continue
                
                # è·å–top-kç»“æœ
                actual_k = min(top_k, len(valid_similarities))
                top_similarities, top_indices = torch.topk(similarities, actual_k)
                
                # è¿‡æ»¤æ‰æ— æ•ˆçš„ç»“æœ
                valid_mask = top_similarities != -float('inf')
                if not valid_mask.any():
                    batch_texts.append("")
                    continue
                
                valid_similarities = top_similarities[valid_mask]
                valid_indices = top_indices[valid_mask]
                
                # è®°å½•æœ€é«˜ç›¸ä¼¼åº¦
                all_similarities[batch_idx, region_idx] = valid_similarities[0].item()
                
                # è·å–æ–‡æœ¬æè¿°
                if len(valid_indices) == 1:
                    # å•ä¸ªç»“æœ
                    best_idx = valid_indices[0].item()
                    best_text = region_texts[best_idx]
                    batch_texts.append(best_text)
                else:
                    # å¤šä¸ªç»“æœåˆå¹¶
                    retrieved_descriptions = []
                    indices_cpu = valid_indices.cpu().numpy()
                    
                    for idx in indices_cpu:
                        if 0 <= idx < len(region_texts):
                            retrieved_descriptions.append(region_texts[idx])
                    
                    if retrieved_descriptions:
                        combined_text = " . ".join(retrieved_descriptions)
                        batch_texts.append(combined_text)
                    else:
                        batch_texts.append("")
            
            retrieved_texts.append(batch_texts)
        
        return retrieved_texts, all_similarities
    
    def create_enhanced_prompt(self, source, enhanced_texts, top_sentences=5):
        """åˆ›å»ºå¢å¼ºçš„promptï¼Œä½¿ç”¨æ‰¹é‡å¹¶è¡Œå¤„ç†æé«˜æ•ˆç‡
        
        Args:
            source: åŒ…å«historyç­‰å­—æ®µçš„æºæ•°æ®
            enhanced_texts: æ¯ä¸ªæ ·æœ¬çš„å¢å¼ºæ–‡æœ¬åˆ—è¡¨ [batch_size, list_of_strings]  
            top_sentences: é€‰æ‹©çš„topå¥å­æ•°é‡
            
        Returns:
            dict: å¢å¼ºåçš„sourceæ•°æ®
        """
        # æ£€æŸ¥è¾“å…¥çš„æœ‰æ•ˆæ€§
        if not enhanced_texts or not any(enhanced_texts):
            if self.config and getattr(self.config, 'DEBUG_TEXT_ENHANCEMENT', False):
                print("ğŸ” create_enhanced_prompt: æ²¡æœ‰å¢å¼ºæ–‡æœ¬ï¼Œè¿”å›åŸå§‹source")
            return source
            
        # æ£€æŸ¥historyæ˜¯å¦ä¸ºNoneæˆ–æ— æ•ˆ
        history = source.get("history")
        if history is None:
            return source
            
        # å¤„ç†ä¸åŒç±»å‹çš„historyå¯¹è±¡
        from transformers.tokenization_utils_base import BatchEncoding
        
        if isinstance(history, BatchEncoding):
            if not hasattr(history, 'input_ids') or history.input_ids is None:
                return source
            history_input_ids = history.input_ids
            history_attention_mask = history.attention_mask
        elif isinstance(history, dict):
            if "input_ids" not in history or history["input_ids"] is None:
                return source
            history_input_ids = history["input_ids"]
            history_attention_mask = history["attention_mask"]
        else:
            return source
            
        device = history_input_ids.device
        batch_size = history_input_ids.size(0)
        original_max_length = history_input_ids.size(1)
        
        # ç¬¬ä¸€æ­¥ï¼šæ‰¹é‡æ”¶é›†å’Œå¤„ç†æ‰€æœ‰å¢å¼ºæ–‡æœ¬
        batch_enhanced_texts = []
        sample_has_enhancement = []
        
        for i in range(batch_size):
            sample_enhanced_texts = enhanced_texts[i] if enhanced_texts[i] else []
            
            if sample_enhanced_texts:
                # å…¨å±€é€‰æ‹©top-Nå¥å­
                enhanced_scores = []
                for text_score_pair in sample_enhanced_texts:
                    if isinstance(text_score_pair, tuple) and len(text_score_pair) == 2:
                        text, score = text_score_pair
                        enhanced_scores.append((text.strip(), float(score)))
                
                if enhanced_scores:
                    # æŒ‰ç½®ä¿¡åº¦æ’åºå¹¶é€‰æ‹©top-N
                    enhanced_scores.sort(key=lambda x: x[1], reverse=True)
                    selected_texts = [text for text, _ in enhanced_scores[:top_sentences]]
                    combined_enhanced_text = " ".join(selected_texts)
                    batch_enhanced_texts.append(combined_enhanced_text)
                    sample_has_enhancement.append(True)
                else:
                    batch_enhanced_texts.append("")
                    sample_has_enhancement.append(False)
            else:
                batch_enhanced_texts.append("")
                sample_has_enhancement.append(False)
        
        # ç¬¬äºŒæ­¥ï¼šæ‰¹é‡tokenizationï¼ˆåªå¯¹éç©ºæ–‡æœ¬è¿›è¡Œï¼‰
        non_empty_texts = [text for text in batch_enhanced_texts if text.strip()]
        
        # åˆ›å»ºå¢å¼ºæ–‡æœ¬ç´¢å¼•æ˜ å°„ï¼Œä¿®å¤ç´¢å¼•ç®¡ç†é—®é¢˜
        enhanced_text_map = {}  # æ˜ å°„batch_index -> enhanced_encoding_index
        enhanced_encoding_idx = 0
        
        if non_empty_texts:
            # æ‰¹é‡ç¼–ç æ‰€æœ‰å¢å¼ºæ–‡æœ¬
            enhanced_encodings = self.tokenizer(
                non_empty_texts,
                add_special_tokens=False,
                return_tensors="pt",
                padding=True,  # æ‰¹é‡padding
                truncation=True,
                max_length=100  # é™åˆ¶å¢å¼ºæ–‡æœ¬é•¿åº¦
            ).to(device)
            
            # å»ºç«‹ç´¢å¼•æ˜ å°„ï¼Œç¡®ä¿ç´¢å¼•æ­£ç¡®å¯¹åº”
            for i in range(batch_size):
                if sample_has_enhancement[i] and batch_enhanced_texts[i].strip():
                    enhanced_text_map[i] = enhanced_encoding_idx
                    enhanced_encoding_idx += 1
            
            # ä½¿ç”¨æ›´åˆé€‚çš„åˆ†éš”ç¬¦
            separator_text = " [SEP] "  # ä½¿ç”¨æ›´æ˜ç¡®çš„åˆ†éš”ç¬¦ï¼Œè€Œä¸æ˜¯ç®€å•çš„ç‚¹å·
            separator_encoding = self.tokenizer(
                separator_text,
                add_special_tokens=False,
                return_tensors="pt",
                padding=False,
                truncation=False
            )
            separator_ids = separator_encoding["input_ids"].squeeze(0).to(device)  # [sep_len]
        
        # ç¬¬ä¸‰æ­¥ï¼šæ‰¹é‡å¤„ç†tokenæ‹¼æ¥å’Œpadding
        enhanced_input_ids_list = []
        enhanced_attention_mask_list = []
        
        # ä¸ºäº†å¹¶è¡Œå¤„ç†ï¼Œé¢„å…ˆè®¡ç®—æ‰€æœ‰æ ·æœ¬çš„å®é™…é•¿åº¦
        actual_lengths = history_attention_mask.sum(dim=1)  # [batch_size]
        
        for i in range(batch_size):
            # è·å–å½“å‰æ ·æœ¬çš„åŸå§‹history tokens
            actual_length = actual_lengths[i].item()
            actual_history_ids = history_input_ids[i, :actual_length]  # [actual_len]
            
            if sample_has_enhancement[i] and batch_enhanced_texts[i].strip() and i in enhanced_text_map:
                # æœ‰å¢å¼ºæ–‡æœ¬çš„æ ·æœ¬ï¼Œä½¿ç”¨ä¿®å¤åçš„ç´¢å¼•æ˜ å°„
                encoding_idx = enhanced_text_map[i]
                enhanced_ids = enhanced_encodings.input_ids[encoding_idx]  # [enhanced_len]
                enhanced_mask = enhanced_encodings.attention_mask[encoding_idx]  # [enhanced_len]
                
                # ç§»é™¤paddingï¼ˆåªå–æœ‰æ•ˆéƒ¨åˆ†ï¼‰
                enhanced_actual_length = enhanced_mask.sum().item()
                enhanced_ids = enhanced_ids[:enhanced_actual_length]
                
                # æ‹¼æ¥ï¼šoriginal_history + separator + enhanced_text
                combined_ids = torch.cat([actual_history_ids, separator_ids, enhanced_ids])
            else:
                # æ²¡æœ‰å¢å¼ºæ–‡æœ¬çš„æ ·æœ¬ï¼Œä¿æŒåŸå§‹
                combined_ids = actual_history_ids
            
            # é™åˆ¶æœ€å¤§é•¿åº¦å¹¶padding
            max_allowed_length = original_max_length + 50  # å…è®¸é€‚å½“æ‰©å±•
            if combined_ids.size(0) > max_allowed_length:
                combined_ids = combined_ids[:max_allowed_length]
            
            # åˆ›å»ºattention mask
            combined_attention_mask = torch.ones(combined_ids.size(0), dtype=torch.long, device=device)
            
            # Paddingåˆ°ç»Ÿä¸€é•¿åº¦
            if combined_ids.size(0) < max_allowed_length:
                pad_length = max_allowed_length - combined_ids.size(0)
                pad_token_id = self.tokenizer.pad_token_id if self.tokenizer.pad_token_id is not None else 0
                
                combined_ids = torch.cat([
                    combined_ids,
                    torch.full((pad_length,), pad_token_id, dtype=torch.long, device=device)
                ])
                combined_attention_mask = torch.cat([
                    combined_attention_mask,
                    torch.zeros(pad_length, dtype=torch.long, device=device)
                ])
            
            enhanced_input_ids_list.append(combined_ids)
            enhanced_attention_mask_list.append(combined_attention_mask)
        
        # ç¬¬å››æ­¥ï¼šæ‰¹é‡è½¬æ¢ä¸ºtensor
        enhanced_input_ids = torch.stack(enhanced_input_ids_list)  # [B, max_length]
        enhanced_attention_mask = torch.stack(enhanced_attention_mask_list)  # [B, max_length]
        
        # æ›´æ–°sourceä¸­çš„history
        enhanced_source = source.copy()
        
        # æ ¹æ®åŸå§‹historyçš„ç±»å‹åˆ›å»ºç›¸åº”çš„è¿”å›æ ¼å¼
        if isinstance(history, BatchEncoding):
            enhanced_source["history"] = BatchEncoding({
                "input_ids": enhanced_input_ids,
                "attention_mask": enhanced_attention_mask
            })
        else:
            enhanced_source["history"] = {
                "input_ids": enhanced_input_ids,
                "attention_mask": enhanced_attention_mask
            }
        
        return enhanced_source
    
    def extract_text_features(self, visual_features, query_image_ids=None, similarity_threshold=0.5, top_k=1):
        """
        æå–æ–‡æœ¬ç‰¹å¾ç”¨äºcross-attention - æ¯ä¸ªè§£å‰–åŒºåŸŸé€‰æ‹©top1
        
        Args:
            visual_features: è§†è§‰ç‰¹å¾å¼ é‡ [batch_size, num_regions, feature_dim]
            query_image_ids: æŸ¥è¯¢å›¾åƒIDåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: æ¯ä¸ªåŒºåŸŸè¿”å›çš„topç›¸ä¼¼æ ·æœ¬æ•°
            
        Returns:
            text_features: æ–‡æœ¬ç‰¹å¾å¼ é‡ [batch_size, num_regions, hidden_dim]
            valid_mask: æœ‰æ•ˆåŒºåŸŸæ©ç  [batch_size, num_regions]
        """
        if not self.enabled:
            return None, None
            
        # æ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ç‰¹å¾å’Œå¯¹åº”çš„æ–‡æœ¬
        retrieved_texts, similarity_scores = self.retrieve_similar_visual_features(
            visual_features, 
            query_image_ids,
            top_k=top_k
        )
        
        if not retrieved_texts:
            return None, None
            
        batch_size = len(retrieved_texts)
        num_regions = len(retrieved_texts[0]) if retrieved_texts else 29  # é»˜è®¤29ä¸ªåŒºåŸŸ
        device = visual_features.device
        
        # å°†similarity_scoresè½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¤„ç†
        if isinstance(similarity_scores, torch.Tensor):
            similarity_scores_np = similarity_scores.cpu().numpy()
        else:
            similarity_scores_np = np.array(similarity_scores)
        
        # ä¸ºæ¯ä¸ªåŒºåŸŸæ”¶é›†æ–‡æœ¬
        all_texts = []
        text_indices = []  # è®°å½•æ¯ä¸ªæ–‡æœ¬å¯¹åº”çš„batchå’ŒåŒºåŸŸç´¢å¼•
        valid_mask = torch.zeros(batch_size, num_regions, dtype=torch.bool, device=device)
        
        for batch_idx in range(batch_size):
            sample_texts = retrieved_texts[batch_idx]
            sample_scores = similarity_scores_np[batch_idx]
            
            for region_idx, text in enumerate(sample_texts):
                region_score = sample_scores[region_idx]
                
                # åªæ·»åŠ è¶…è¿‡é˜ˆå€¼ä¸”éç©ºçš„æ–‡æœ¬
                if region_score >= similarity_threshold and text and text.strip():
                    all_texts.append(text.strip())
                    text_indices.append((batch_idx, region_idx))
                    valid_mask[batch_idx, region_idx] = True
                else:
                    # å¯¹äºæ— æ•ˆåŒºåŸŸï¼Œæ·»åŠ å ä½ç¬¦æ–‡æœ¬
                    all_texts.append("")
                    text_indices.append((batch_idx, region_idx))
        
        if not any(text.strip() for text in all_texts):
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ–‡æœ¬ï¼Œè¿”å›é›¶å¼ é‡
            zero_features = torch.zeros(batch_size, num_regions, 768, device=device)
            return zero_features, valid_mask
        
        # æ‰¹é‡tokenization
        with torch.no_grad():
            encoded = self.tokenizer(
                all_texts,
                add_special_tokens=True,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=64  # é™åˆ¶å•å¥é•¿åº¦
            ).to(device)
            
            # è·å–æ–‡æœ¬åµŒå…¥ - ä½¿ç”¨æ›´robustçš„æ–¹æ³•è·å–åµŒå…¥å±‚
            embed_layer = None
            hidden_dim = 768
            
            # å°è¯•å¤šç§æ–¹å¼è·å–åµŒå…¥å±‚
            if hasattr(self.tokenizer, 'model') and hasattr(self.tokenizer.model, 'get_input_embeddings'):
                # å¯¹äºTransformersæ¨¡å‹ï¼ˆå¦‚BERTï¼‰
                embed_layer = self.tokenizer.model.get_input_embeddings()
                hidden_dim = embed_layer.embedding_dim
            elif hasattr(self.tokenizer, 'get_input_embeddings'):
                # ç›´æ¥ä»tokenizerè·å–
                embed_layer = self.tokenizer.get_input_embeddings()
                hidden_dim = embed_layer.embedding_dim
            else:
                # å¦‚æœæ— æ³•è·å–åµŒå…¥å±‚ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–çš„åµŒå…¥å±‚
                print("è­¦å‘Š: æ— æ³•è·å–tokenizerçš„åµŒå…¥å±‚ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
                vocab_size = len(self.tokenizer.vocab) if hasattr(self.tokenizer, 'vocab') else 30522
                embed_layer = nn.Embedding(vocab_size, hidden_dim).to(device)
                # ä½¿ç”¨Xavieråˆå§‹åŒ–
                nn.init.xavier_uniform_(embed_layer.weight)
            
            # è®¡ç®—æ¯ä¸ªå¥å­çš„å¹³å‡åµŒå…¥ï¼ˆå¿½ç•¥padding tokensï¼‰
            input_ids = encoded['input_ids']  # [num_texts, seq_len]
            attention_mask = encoded['attention_mask']  # [num_texts, seq_len]
            
            # è·å–tokenåµŒå…¥
            token_embeds = embed_layer(input_ids)  # [num_texts, seq_len, hidden_dim]
            
            # è®¡ç®—æ¯ä¸ªå¥å­çš„maskedå¹³å‡åµŒå…¥
            masked_embeds = token_embeds * attention_mask.unsqueeze(-1).float()
            sentence_embeds = masked_embeds.sum(dim=1) / attention_mask.sum(dim=1, keepdim=True).float()  # [num_texts, hidden_dim]
            
            # å¦‚æœåµŒå…¥ç»´åº¦ä¸æ˜¯768ï¼Œéœ€è¦è¿›è¡ŒæŠ•å½±
            if hidden_dim != 768:
                if not hasattr(self, 'text_embed_projection'):
                    self.text_embed_projection = nn.Linear(hidden_dim, 768).to(device)
                sentence_embeds = self.text_embed_projection(sentence_embeds)
        
        # é‡æ–°ç»„ç»‡ä¸ºbatchæ ¼å¼
        text_features = torch.zeros(batch_size, num_regions, 768, device=device)
        
        for i, (batch_idx, region_idx) in enumerate(text_indices):
            text_features[batch_idx, region_idx] = sentence_embeds[i]
        
        return text_features, valid_mask

    def forward(self, visual_features, history_text=None, query_image_ids=None, similarity_threshold=0.5, top_k=1, top_sentences=5, return_features=False):
        """
        å‰å‘ä¼ æ’­ï¼šæ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ç‰¹å¾å¹¶ç”Ÿæˆå¢å¼ºæ–‡æœ¬æˆ–æ–‡æœ¬ç‰¹å¾
        
        Args:
            visual_features: è§†è§‰ç‰¹å¾å¼ é‡ [batch_size, num_regions, feature_dim]
            history_text: å†å²æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œä¸ºäº†å‘åå…¼å®¹ï¼‰
            query_image_ids: æŸ¥è¯¢å›¾åƒIDåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            top_k: æ¯ä¸ªåŒºåŸŸè¿”å›çš„topç›¸ä¼¼æ ·æœ¬æ•°
            top_sentences: å…¨å±€é€‰æ‹©çš„topå¥å­æ•°ï¼ˆä»…åœ¨return_features=Falseæ—¶ä½¿ç”¨ï¼‰
            return_features: æ˜¯å¦è¿”å›æ–‡æœ¬ç‰¹å¾è€Œéæ–‡æœ¬å­—ç¬¦ä¸²
            
        Returns:
            å¦‚æœreturn_features=True: (text_features [B, num_regions, 768], valid_mask [B, num_regions])
            å¦‚æœreturn_features=False: enhanced_texts (æ¯ä¸ªæ ·æœ¬çš„(æ–‡æœ¬, åˆ†æ•°)å…ƒç»„åˆ—è¡¨)
        """
        if not self.enabled:
            if return_features:
                return None, None
            else:
                return None
        
        if return_features:
            return self.extract_text_features(
                visual_features, 
                query_image_ids, 
                similarity_threshold, 
                top_k
            )
        else:
            # åŸæœ‰çš„æ–‡æœ¬è¿”å›é€»è¾‘
            # æ£€ç´¢ç›¸ä¼¼çš„è§†è§‰ç‰¹å¾å’Œå¯¹åº”çš„æ–‡æœ¬
            retrieved_texts, similarity_scores = self.retrieve_similar_visual_features(
                visual_features, 
                query_image_ids,
                top_k=top_k
            )
            
            # è½¬æ¢ä¸º(æ–‡æœ¬, åˆ†æ•°)å…ƒç»„åˆ—è¡¨æ ¼å¼
            enhanced_texts = []
            batch_size = len(retrieved_texts) if retrieved_texts else 0
            
            if batch_size == 0:
                return None
                
            # å°†similarity_scoresè½¬æ¢ä¸ºnumpyæ•°ç»„ä»¥ä¾¿å¤„ç†
            if isinstance(similarity_scores, torch.Tensor):
                similarity_scores_np = similarity_scores.cpu().numpy()
            else:
                similarity_scores_np = np.array(similarity_scores)
            
            for batch_idx in range(batch_size):
                sample_texts = retrieved_texts[batch_idx]
                sample_scores = similarity_scores_np[batch_idx]
                
                # æ”¶é›†å½“å‰æ ·æœ¬æ‰€æœ‰åŒºåŸŸçš„(æ–‡æœ¬, åˆ†æ•°)å¯¹
                sample_enhanced_texts = []
                
                for region_idx, text in enumerate(sample_texts):
                    region_score = sample_scores[region_idx]
                    
                    # åªæ·»åŠ è¶…è¿‡é˜ˆå€¼çš„æ–‡æœ¬
                    if region_score >= similarity_threshold and text and text.strip():
                        sample_enhanced_texts.append((text.strip(), region_score))
                
                enhanced_texts.append(sample_enhanced_texts)
            
            return enhanced_texts