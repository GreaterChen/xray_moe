import torch
import torch.nn as nn
import torch.nn.functional as F
import gc
import pickle
import os
from models.fast_rcnn_classifier import DetectionOnlyFastRCNN
from models.vit import MedicalVisionTransformer
from utils import analyze_gpu_memory

from models.negativa_sample_pool import NegativeSamplePool
from models.text_enhancement import AnatomicalTextEnhancer


class MOE(nn.Module):
    def __init__(
        self,
        config,
        object_detector=None,
        image_encoder=None,
        modality_fusion=None,
        findings_decoder=None,
        cxr_bert=None,
    ):
        super(MOE, self).__init__()

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.object_detector = object_detector
        self.image_encoder = image_encoder
        self.modality_fusion = modality_fusion
        self.findings_decoder = findings_decoder
        self.cxr_bert = cxr_bert
        # ä¿å­˜å‚æ•°é…ç½®
        self.config = config
        
        # æ·»åŠ æ£€æµ‹ç»“æœç¼“å­˜æ”¯æŒ
        self.use_detection_cache = False
        self.detection_cache = {}
        
        self.visual_projection = nn.Linear(768, 768)
        self.text_projection = nn.Linear(768, 768)
        
        # ä¸ºåŒºåŸŸçº§åˆ«å¯¹æ¯”å­¦ä¹ æ·»åŠ ç‹¬ç«‹çš„æŠ•å½±å±‚
        self.region_visual_projection = nn.Linear(768, 768)
        self.region_text_projection = nn.Linear(768, 768)
        
        # æ·»åŠ Cross-Attentionæ¨¡å—ç”¨äºæ–‡æœ¬å¢å¼ºï¼ˆåªåœ¨æ”¯æŒçš„é˜¶æ®µä¸­åˆå§‹åŒ–ï¼‰
        enable_text_enhancement = getattr(config, 'ENABLE_TEXT_ENHANCEMENT', False)
        use_cross_attention = getattr(config, 'TEXT_ENHANCEMENT_USE_CROSS_ATTENTION', True)
        
        if enable_text_enhancement and use_cross_attention:
            supported_phases = getattr(config, 'TEXT_ENHANCEMENT_PHASES', ["FINETUNE_BERT"])
            current_phase = getattr(config, 'PHASE', None)
            
            if current_phase in supported_phases:
                # ä»é…ç½®ä¸­è·å–Cross-Attentionå‚æ•°
                num_heads = getattr(config, 'TEXT_ENHANCEMENT_CROSS_ATTN_HEADS', 12)
                dropout_rate = getattr(config, 'TEXT_ENHANCEMENT_CROSS_ATTN_DROPOUT', 0.1)
                
                # Cross-Attentionæ¨¡å—ï¼šæ–‡æœ¬ç‰¹å¾attend toè§†è§‰ç‰¹å¾
                self.text_to_visual_cross_attn = nn.MultiheadAttention(
                    embed_dim=768, 
                    num_heads=num_heads, 
                    dropout=dropout_rate, 
                    batch_first=True
                )
                
                print(f"âœ… Cross-Attentionæ–‡æœ¬å¢å¼ºæ¨¡å—å·²åœ¨ {current_phase} é˜¶æ®µåˆå§‹åŒ–")
                print(f"   - æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
                print(f"   - Dropoutç‡: {dropout_rate}")
            else:
                print(f"ğŸ“ å½“å‰é˜¶æ®µ {current_phase} ä¸åœ¨Cross-Attentionæ–‡æœ¬å¢å¼ºæ”¯æŒé˜¶æ®µ {supported_phases} ä¸­")
        elif enable_text_enhancement and not use_cross_attention:
            print("ğŸ“ æ–‡æœ¬å¢å¼ºåŠŸèƒ½å¯ç”¨ï¼Œä½†ä½¿ç”¨ä¼ ç»Ÿæ‹¼æ¥æ–¹å¼ï¼ˆéCross-Attentionï¼‰")
        else:
            print("ğŸ“ Cross-Attentionæ–‡æœ¬å¢å¼ºåŠŸèƒ½æœªå¯ç”¨")
        
        # åªåœ¨PRETRAIN_VITé˜¶æ®µåŠ è½½è´Ÿæ ·æœ¬æ± 
        if config.PHASE == "PRETRAIN_VIT":
            self.negative_pool = NegativeSamplePool(
                num_diseases=config.NUM_DISEASES if hasattr(config, "NUM_DISEASES") else 14
            )
            self.negative_pool.load(config.NEGATIVE_POOL_DIR)
        
        # åˆå§‹åŒ–æ–‡æœ¬å¢å¼ºæ¨¡å—ï¼ˆæ ¹æ®é…ç½®å†³å®šæ˜¯å¦å¯ç”¨ï¼‰
        self.text_enhancer = None
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ–‡æœ¬å¢å¼ºåŠŸèƒ½
        enable_text_enhancement = getattr(config, 'ENABLE_TEXT_ENHANCEMENT', False)
        
        if enable_text_enhancement:
            # æ£€æŸ¥æ˜¯å¦åœ¨æ”¯æŒçš„é˜¶æ®µ
            supported_phases = getattr(config, 'TEXT_ENHANCEMENT_PHASES', ["FINETUNE_BERT"])
            current_phase = getattr(config, 'PHASE', None)
            
            if current_phase in supported_phases:
                try:
                    # è·å–tokenizerï¼ˆä»findings_decoderä¸­è·å–ï¼‰
                    tokenizer = None
                    if hasattr(self.findings_decoder, 'tokenizer'):
                        tokenizer = self.findings_decoder.tokenizer
                    elif hasattr(self.findings_decoder, 'decoder') and hasattr(self.findings_decoder.decoder, 'tokenizer'):
                        tokenizer = self.findings_decoder.decoder.tokenizer
                    else:
                        print("âš ï¸  è­¦å‘Š: æ— æ³•è·å–tokenizerï¼Œæ–‡æœ¬å¢å¼ºåŠŸèƒ½å°†è¢«ç¦ç”¨")
                    
                    if tokenizer is not None:
                        self.text_enhancer = AnatomicalTextEnhancer(
                            tokenizer=tokenizer,
                            visual_projection=self.region_visual_projection,
                            text_projection=self.region_text_projection,
                            device="cuda" if torch.cuda.is_available() else "cpu",
                            config=config  # ä¼ é€’é…ç½®å‚æ•°
                        )
                        
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸå¯ç”¨
                        if self.text_enhancer.enabled:
                            print(f"âœ… æ–‡æœ¬å¢å¼ºæ¨¡å—å·²åœ¨ {current_phase} é˜¶æ®µå¯ç”¨")
                        else:
                            print(f"âŒ æ–‡æœ¬å¢å¼ºæ¨¡å—å¯åŠ¨å¤±è´¥")
                            self.text_enhancer = None
                except Exception as e:
                    print(f"âŒ æ–‡æœ¬å¢å¼ºæ¨¡å—åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.text_enhancer = None
            else:
                print(f"ğŸ“ å½“å‰é˜¶æ®µ {current_phase} ä¸åœ¨æ–‡æœ¬å¢å¼ºæ”¯æŒé˜¶æ®µ {supported_phases} ä¸­")
        else:
            print("ğŸ“ æ–‡æœ¬å¢å¼ºåŠŸèƒ½æœªåœ¨é…ç½®ä¸­å¯ç”¨")



    def forward(
        self,
        image,
        bbox_targets=None,
        findings=None,
        history=None,
        targets=None,
        label=None,
        phase="UNK",
        current_epoch=0,
        total_epochs=20,
        mode="train",
        image_ids=None,  # æ·»åŠ image_idså‚æ•°ç”¨äºæ–‡æœ¬å¢å¼º
        use_consistent_eval=False,  # æ–°å¢å‚æ•°ï¼šæ˜¯å¦åœ¨æµ‹è¯•æ—¶ä¿æŒè®­ç»ƒæ¨¡å¼ä»¥ç¡®ä¿ä¸€è‡´æ€§
        anatomical_embeddings_batch=None,  # æ–°å¢ï¼šæ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„è§£å‰–åŒºåŸŸåµŒå…¥
        **kwargs
    ):
        # åœ¨è¿™é‡Œå®ç°å‰å‘ä¼ æ’­é€»è¾‘
        if phase == "TRAIN_DETECTION":
            return self.object_detector(image, bbox_targets)
        elif phase == "PRETRAIN_VIT":
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç›®æ ‡æ£€æµ‹å™¨æå–åŒºåŸŸç‰¹å¾
            with torch.no_grad():  # åœ¨é˜¶æ®µ2å†»ç»“ç›®æ ‡æ£€æµ‹å™¨
                detection_outputs = self.object_detector(
                    image,
                    bbox_targets,
                    current_epoch=current_epoch,
                    total_epochs=total_epochs,
                )
                region_features = detection_outputs["region_features"]
                region_detected = detection_outputs["region_detected"]

            # ç¬¬äºŒæ­¥ï¼šé€šè¿‡ViTå¤„ç†åŒºåŸŸç‰¹å¾
            # å¦‚æœæ˜¯æµ‹è¯•æ¨¡å¼ä¸”è¦æ±‚ä¸€è‡´æ€§è¯„ä¼°ï¼Œä¸´æ—¶åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼
            original_training = self.training
            if mode != "train" and use_consistent_eval:
                self.train()  # åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼ä»¥ä¿æŒdropoutç­‰è¡Œä¸ºä¸€è‡´
                print("âš ï¸  ä¸ºäº†ä¸€è‡´æ€§æ¯”è¾ƒï¼Œåœ¨æµ‹è¯•æ—¶ä½¿ç”¨è®­ç»ƒæ¨¡å¼ï¼ˆdropoutç­‰ä¿æŒæ¿€æ´»ï¼‰")
            
            image_encoder_outputs = self.image_encoder(
                region_features, region_detected=region_detected, image_labels=label, use_moe=False
            )

            # è·å–ViTçš„å®Œæ•´è¾“å‡ºï¼Œåœ¨éœ€è¦æ—¶é€šè¿‡ç´¢å¼•æå–cls_token
            visual_features = image_encoder_outputs["visual_features"]  # [B, 1+num_regions, hidden_size]
            cls_token = visual_features[:, 0]  # æå–cls_token [B, hidden_size]

            # æ¢å¤åŸå§‹è®­ç»ƒçŠ¶æ€
            if mode != "train" and use_consistent_eval:
                self.train(original_training)

            if mode == "train":
                # ä½¿ç”¨CXR-BERTç¼–ç findingsï¼Œè·å–text_cls_token
                with torch.no_grad():  # å†»ç»“CXR-BERT
                    text_cls_token = self.cxr_bert(findings)

                # å°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾æ˜ å°„åˆ°å…±äº«ç©ºé—´
                mapped_visual_cls = self.visual_projection(cls_token)
                mapped_text_cls = self.text_projection(text_cls_token)

                # è·å–å½“å‰æ‰¹æ¬¡çš„ç–¾ç—…æ ‡ç­¾/é¢„æµ‹
                disease_labels = label

                # è®¡ç®—å…¨å±€å¯¹æ¯”æŸå¤±(LTC)
                if disease_labels is not None and self.negative_pool is not None:
                    # ä½¿ç”¨negative poolè·å–å›°éš¾è´Ÿæ ·æœ¬
                    batch_size = mapped_visual_cls.size(0)
                    # å›ºå®šè´Ÿæ ·æœ¬æ•°é‡ï¼Œé¿å…batch sizeå½±å“
                    fixed_neg_samples = 63  # å›ºå®šä½¿ç”¨63ä¸ªè´Ÿæ ·æœ¬
                    
                    # ä¸ºæ¯ä¸ªæ ·æœ¬è·å–å¯¹åº”çš„è´Ÿæ ·æœ¬å¹¶æ˜ å°„åˆ°å…±äº«ç©ºé—´
                    negative_samples = self.negative_pool.get_negative_samples_batch(
                        disease_labels, k=fixed_neg_samples
                    )
                    mapped_negative_samples = self.text_projection(
                        negative_samples
                    )  # [B, K, hidden_size]

                    # ä½¿ç”¨å›°éš¾è´Ÿæ ·æœ¬è®¡ç®—å¯¹æ¯”æŸå¤±
                    ltc_loss = self.compute_global_ltc_loss(
                        mapped_visual_cls, mapped_text_cls, mapped_negative_samples
                    )
                    # print(f"[TRAIN] ä½¿ç”¨å…¨å±€è´Ÿæ ·æœ¬æ± è®¡ç®—LTC loss: {ltc_loss.item():.4f} (neg_samples={fixed_neg_samples})")
                else:
                    # å¦‚æœæ²¡æœ‰è´Ÿæ ·æœ¬æ± ï¼Œä½¿ç”¨æ‰¹å†…å¯¹æ¯”
                    ltc_loss = self.compute_batch_ltc_loss(
                        mapped_visual_cls, mapped_text_cls
                    )
                    # print(f"[TRAIN] ä½¿ç”¨æ‰¹å†…å¯¹æ¯”è®¡ç®—LTC loss: {ltc_loss.item():.4f}")

                # è®¡ç®—åŒºåŸŸçº§åˆ«çš„ITCæŸå¤±ï¼ˆæ–°å¢ï¼‰
                region_itc_loss = None
                if getattr(self.config, 'ENABLE_REGION_ITC', True):
                    region_itc_loss = self.compute_region_itc_loss(
                        visual_features, region_detected, anatomical_embeddings_batch, image_ids
                    )

                # è¿”å›åŒ…å«LTCæŸå¤±å’ŒåŒºåŸŸITCæŸå¤±çš„ç»“æœ
                results = {
                    "disease_preds": image_encoder_outputs["disease_preds"],
                    "final_disease_preds": image_encoder_outputs["final_disease_preds"],
                    "ltc_loss": ltc_loss,
                    "region_itc_loss": region_itc_loss,
                    "cls_loss": image_encoder_outputs["loss"],
                }
                return results

            # å¦‚æœæ˜¯è¯„ä¼°/æ¨ç†æ¨¡å¼
            else:
                # ä¸ºæµ‹è¯•æ¨¡å¼è®¡ç®—ltc_losså’Œcls_loss
                with torch.no_grad():  # ç¡®ä¿ä¸è®¡ç®—æ¢¯åº¦
                    # ä½¿ç”¨CXR-BERTç¼–ç findingsï¼Œè·å–text_cls_token
                    text_cls_token = (
                        self.cxr_bert(findings) if findings is not None else None
                    )

                    # åªæœ‰å½“findingså¯ç”¨æ—¶æ‰è®¡ç®—ltc_loss
                    ltc_loss = None
                    if text_cls_token is not None:
                        # å°†æ–‡æœ¬å’Œè§†è§‰ç‰¹å¾æ˜ å°„åˆ°å…±äº«ç©ºé—´
                        mapped_visual_cls = self.visual_projection(cls_token)
                        mapped_text_cls = self.text_projection(text_cls_token)

                        # æµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„lossè®¡ç®—æ–¹å¼
                        disease_labels = label
                        if disease_labels is not None and self.negative_pool is not None:
                            # ä½¿ç”¨negative poolè·å–å›°éš¾è´Ÿæ ·æœ¬ï¼ˆä¸è®­ç»ƒæ—¶ç›¸åŒï¼‰
                            batch_size = mapped_visual_cls.size(0)
                            # å›ºå®šè´Ÿæ ·æœ¬æ•°é‡ï¼Œé¿å…batch sizeå½±å“
                            fixed_neg_samples = 64  # å›ºå®šä½¿ç”¨64ä¸ªè´Ÿæ ·æœ¬
                            
                            # ä¸ºæ¯ä¸ªæ ·æœ¬è·å–å¯¹åº”çš„è´Ÿæ ·æœ¬å¹¶æ˜ å°„åˆ°å…±äº«ç©ºé—´
                            negative_samples = self.negative_pool.get_negative_samples_batch(
                                disease_labels, k=fixed_neg_samples
                            )
                            mapped_negative_samples = self.text_projection(
                                negative_samples
                            )  # [B, K, hidden_size]

                            # ä½¿ç”¨å›°éš¾è´Ÿæ ·æœ¬è®¡ç®—å¯¹æ¯”æŸå¤±
                            ltc_loss = self.compute_global_ltc_loss(
                                mapped_visual_cls, mapped_text_cls, mapped_negative_samples
                            )
                            # print(f"[TEST] ä½¿ç”¨å…¨å±€è´Ÿæ ·æœ¬æ± è®¡ç®—LTC loss: {ltc_loss.item():.4f} (neg_samples={fixed_neg_samples})")
                        else:
                            # å¦‚æœæ²¡æœ‰è´Ÿæ ·æœ¬æ± ï¼Œä½¿ç”¨æ‰¹å†…å¯¹æ¯”
                            ltc_loss = self.compute_batch_ltc_loss(
                                mapped_visual_cls, mapped_text_cls
                            )
                            # print(f"[TEST] ä½¿ç”¨æ‰¹å†…å¯¹æ¯”è®¡ç®—LTC loss: {ltc_loss.item():.4f}")

                    # è·å–åˆ†ç±»æŸå¤±
                    cls_loss = image_encoder_outputs.get("loss", None)
                    
                    # è®¡ç®—åŒºåŸŸçº§åˆ«çš„ITCæŸå¤±ï¼ˆæµ‹è¯•æ¨¡å¼ï¼‰
                    region_itc_loss = None
                    if getattr(self.config, 'ENABLE_REGION_ITC', True):
                        region_itc_loss = self.compute_region_itc_loss(
                            visual_features, region_detected, anatomical_embeddings_batch, image_ids
                        )

                # è¿”å›ç®€åŒ–çš„ç»“æœï¼ŒåªåŒ…å«éœ€è¦çš„å­—æ®µ
                return {
                    "disease_preds": image_encoder_outputs["disease_preds"],
                    "final_disease_preds": image_encoder_outputs["final_disease_preds"],
                    "ltc_loss": ltc_loss,
                    "region_itc_loss": region_itc_loss,
                    "cls_loss": cls_loss,   
                }

        elif phase == "INFER_BERT":
            # INFER_BERTé˜¶æ®µï¼šä½¿ç”¨å®Œæ•´çš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼ˆå’ŒFINETUNE_BERTç±»ä¼¼ä½†åœ¨æ¨ç†æ¨¡å¼ï¼‰
            with torch.no_grad():
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç¼“å­˜çš„æ£€æµ‹ç»“æœ
                if self.use_detection_cache and hasattr(self, 'detection_cache'):
                    # ä»ç¼“å­˜ä¸­è·å–æ£€æµ‹ç»“æœ
                    batch_size = image.shape[0]
                    device = image.device
                    
                    # è·å–å½“å‰batchçš„image_ids
                    if 'image_ids' in kwargs:
                        image_ids = kwargs['image_ids']
                    elif image_ids is not None:
                        pass  # ä½¿ç”¨ä¼ å…¥çš„image_ids
                    else:
                        # å¦‚æœæ²¡æœ‰image_idsï¼Œå›é€€åˆ°æ­£å¸¸æ£€æµ‹
                        print("âš ï¸  è­¦å‘Šï¼šç¼“å­˜æ¨¡å¼ä¸‹æœªæä¾›image_idsï¼Œå›é€€åˆ°æ­£å¸¸æ£€æµ‹")
                        detection_outputs = self.object_detector(
                            image, bbox_targets, current_epoch=current_epoch, total_epochs=total_epochs,
                        )
                        region_features = detection_outputs["region_features"]
                        region_detected = detection_outputs["region_detected"]
                    
                    if image_ids is not None:
                        # ä»ç¼“å­˜æ„å»ºbbox targetsï¼ˆä½œä¸º"ground truth"ï¼‰
                        cached_targets = []
                        
                        for i, img_id in enumerate(image_ids):
                            if img_id in self.detection_cache:
                                cache_data = self.detection_cache[img_id]
                                # æ„å»ºç›®æ ‡æ ¼å¼ï¼Œä½¿ç”¨ç¼“å­˜çš„bboxä½œä¸º"ground truth"
                                target = {
                                    "boxes": torch.tensor(cache_data["boxes"], device=device, dtype=torch.float32),
                                    "labels": torch.tensor(cache_data["labels"], device=device, dtype=torch.long),
                                    "image_id": torch.tensor(i, device=device),
                                    "area": torch.tensor([0.0] * len(cache_data["boxes"]), device=device),  # å ä½ç¬¦
                                    "iscrowd": torch.tensor([0] * len(cache_data["boxes"]), device=device, dtype=torch.long)
                                }
                                cached_targets.append(target)
                            else:
                                print(f"âš ï¸  è­¦å‘Šï¼šå›¾åƒ {img_id} ä¸åœ¨æ£€æµ‹ç¼“å­˜ä¸­")
                                # å›é€€åˆ°å®æ—¶æ£€æµ‹
                                single_detection = self.object_detector.predict_regions(image[i:i+1])
                                target = {
                                    "boxes": single_detection[0]["boxes"],
                                    "labels": single_detection[0]["labels"], 
                                    "image_id": torch.tensor(i, device=device),
                                    "area": torch.tensor([0.0] * len(single_detection[0]["boxes"]), device=device),
                                    "iscrowd": torch.tensor([0] * len(single_detection[0]["boxes"]), device=device, dtype=torch.long)
                                }
                                cached_targets.append(target)
                        
                        # ä½¿ç”¨ç¼“å­˜çš„bboxä½œä¸º"ground truth"æå–ç‰¹å¾ï¼ˆç›¸å½“äºuse_gt=Trueï¼‰
                        detection_outputs = self.object_detector(
                            image,
                            cached_targets,  # ä½¿ç”¨ç¼“å­˜çš„bboxä½œä¸ºtargets
                            current_epoch=current_epoch,
                            total_epochs=total_epochs,
                        )
                        region_features = detection_outputs["region_features"]
                        region_detected = detection_outputs["region_detected"]
                        
                        print(f"âœ… ä½¿ç”¨ {len(image_ids)} ä¸ªæ ·æœ¬çš„ç¼“å­˜bboxæå–ç‰¹å¾")
                else:
                    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç›®æ ‡æ£€æµ‹å™¨æå–åŒºåŸŸç‰¹å¾ï¼ˆå†»ç»“ï¼‰
                    detection_outputs = self.object_detector(
                        image,
                        bbox_targets,
                        current_epoch=current_epoch,
                        total_epochs=total_epochs,
                    )
                    region_features = detection_outputs["region_features"]
                    region_detected = detection_outputs["region_detected"]

                # ç¬¬äºŒæ­¥ï¼šé€šè¿‡ViTå¤„ç†åŒºåŸŸç‰¹å¾ï¼ˆå†»ç»“ï¼‰
                image_encoder_outputs = self.image_encoder(
                    region_features, 
                    region_detected=region_detected, 
                    image_labels=label,
                    phase=phase,  # ä¼ é€’phaseå‚æ•°ç»™ViT
                    use_moe=True
                )
 
                # ç›´æ¥ä½¿ç”¨ViTè¾“å‡ºçš„å®Œæ•´è§†è§‰ç‰¹å¾ï¼ˆå·²åŒ…å«cls_tokenå’Œregionç‰¹å¾ï¼‰
                visual_features = image_encoder_outputs["visual_features"]  # [B, 1+num_regions, hidden_size]

                # ç¬¬ä¸‰æ­¥ï¼šé€šè¿‡BERTè§£ç å™¨è¿›è¡Œæ¨ç†
                if mode == "train":
                    # å¦‚æœæ˜¯è®­ç»ƒæ¨¡å¼ï¼ˆç”¨äºæ„å»ºè´Ÿæ ·æœ¬æ± ç­‰ï¼‰
                    outputs = self.findings_decoder(
                        visual_features=visual_features,
                        history_encoding=history,
                        findings=findings,
                        use_history=True
                    )
                    return outputs
                else:
                    # çº¯æ¨ç†æ¨¡å¼ï¼šåªç”Ÿæˆæ–‡æœ¬
                    generated_texts = self.findings_decoder.generate(
                        visual_features=visual_features,
                        history_encoding=history,
                        use_history=True
                    )
                    return {"findings_text": generated_texts}

        elif phase == "FINETUNE_MISTRAL" or phase == "FINETUNE_LLAMA" or phase == "FINETUNE_BERT":
            # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç›®æ ‡æ£€æµ‹å™¨æå–åŒºåŸŸç‰¹å¾ï¼ˆå†»ç»“ï¼‰
            with torch.no_grad():
                detection_outputs = self.object_detector(
                    image,
                    bbox_targets,
                    current_epoch=current_epoch,
                    total_epochs=total_epochs,
                )
                region_features = detection_outputs["region_features"]
                region_detected = detection_outputs["region_detected"]

            # ç¬¬äºŒæ­¥ï¼šé€šè¿‡ViTå¤„ç†åŒºåŸŸç‰¹å¾
            image_encoder_outputs = self.image_encoder(
                region_features, 
                region_detected=region_detected, 
                image_labels=label,
                phase=phase,  # ä¼ é€’phaseå‚æ•°ç»™ViT
                use_moe=True
            )
 
            # ç›´æ¥ä½¿ç”¨ViTè¾“å‡ºçš„å®Œæ•´è§†è§‰ç‰¹å¾ï¼ˆå·²åŒ…å«cls_tokenå’Œregionç‰¹å¾ï¼‰
            visual_features = image_encoder_outputs["visual_features"]  # [B, 1+num_regions, hidden_size]

            # ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨åŸºäºCross-Attentionçš„æ–‡æœ¬å¢å¼º
            enhanced_visual_features = visual_features  # é»˜è®¤ä½¿ç”¨åŸå§‹è§†è§‰ç‰¹å¾
            enhanced_history = history  # é»˜è®¤ä½¿ç”¨åŸå§‹å†å²æ–‡æœ¬
            
            # æ£€æŸ¥æ˜¯å¦åº”è¯¥åœ¨å½“å‰é˜¶æ®µä½¿ç”¨æ–°çš„Cross-Attentionæ–‡æœ¬å¢å¼º
            should_use_cross_attention = (
                hasattr(self, 'text_to_visual_cross_attn') and
                self.text_enhancer is not None and 
                self.text_enhancer.enabled and
                getattr(self.config, 'ENABLE_TEXT_ENHANCEMENT', False) and
                getattr(self.config, 'TEXT_ENHANCEMENT_USE_CROSS_ATTENTION', True)
            )

            should_use_cross_attention = False
            
            if should_use_cross_attention:
                # æå–åŒºåŸŸç‰¹å¾ï¼ˆå»é™¤CLS tokenï¼‰ç”¨äºæ–‡æœ¬æ£€ç´¢
                region_features = visual_features[:, 1:30, :]  # [batch_size, 29, 768]
                
                # åº”ç”¨Cross-Attentionæ–‡æœ¬å¢å¼ºï¼Œä¸å†å¤„ç†historyæ‹¼æ¥
                enhanced_visual_features = self.apply_text_enhancement(
                    visual_features=visual_features,
                    region_features=region_features,
                    image_ids=image_ids
                )
                # ä½¿ç”¨Cross-Attentionæ—¶ï¼Œhistoryä¿æŒåŸæ ·ï¼Œä¸è¿›è¡Œæ–‡æœ¬æ‹¼æ¥
                enhanced_history = history
            else:
                # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ä¼ ç»Ÿçš„æ–‡æœ¬æ‹¼æ¥å¢å¼ºæ–¹å¼ï¼ˆå‘åå…¼å®¹ï¼‰
                should_use_legacy_enhancement = (
                    self.text_enhancer is not None and 
                    self.text_enhancer.enabled and
                    getattr(self.config, 'ENABLE_TEXT_ENHANCEMENT', False) and
                    not should_use_cross_attention
                )
                
                if should_use_legacy_enhancement:
                    try:
                        # ä»é…ç½®ä¸­è·å–æ–‡æœ¬å¢å¼ºå‚æ•°
                        similarity_threshold = getattr(self.config, 'TEXT_ENHANCEMENT_SIMILARITY_THRESHOLD', 0.5)
                        top_k = getattr(self.config, 'TEXT_ENHANCEMENT_TOP_K', 1)
                        top_sentences = getattr(self.config, 'TEXT_ENHANCEMENT_TOP_SENTENCES', 5)
                        
                        # æå–åŒºåŸŸç‰¹å¾ï¼ˆå»é™¤CLS tokenï¼‰
                        region_features = visual_features[:, 1:30, :]  # [batch_size, 29, 768]
                        
                        # æ£€ç´¢å¢å¼ºæ–‡æœ¬ï¼ˆè¿”å›(æ–‡æœ¬, åˆ†æ•°)å…ƒç»„åˆ—è¡¨ï¼‰
                        enhanced_texts = self.text_enhancer(
                            visual_features=region_features,
                            query_image_ids=image_ids,
                            similarity_threshold=similarity_threshold,
                            top_k=top_k,
                            top_sentences=top_sentences,
                            return_features=False  # ä½¿ç”¨ä¼ ç»Ÿçš„æ–‡æœ¬è¿”å›æ–¹å¼
                        )
                        
                        # ç›´æ¥åœ¨embeddingå±‚é¢å¢å¼ºhistoryï¼ˆé¿å…è§£ç -ç¼–ç å¾€è¿”ï¼‰
                        if enhanced_texts is not None:
                            # æ„é€ åŒ…å«historyçš„sourceå­—å…¸
                            source_dict = {"history": history}
                            
                            # åº”ç”¨embeddingå±‚é¢çš„æ–‡æœ¬å¢å¼º
                            enhanced_source = self.text_enhancer.create_enhanced_prompt(
                                source=source_dict,
                                enhanced_texts=enhanced_texts,
                                top_sentences=top_sentences
                            )
                            
                            # æå–å¢å¼ºåçš„history
                            enhanced_history = enhanced_source["history"]
                        else:
                            enhanced_history = history
                        
                    except Exception as e:
                        print(f"âŒ ä¼ ç»Ÿæ–‡æœ¬å¢å¼ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
                        import traceback
                        traceback.print_exc()
                        # å‡ºé”™æ—¶ä½¿ç”¨åŸå§‹å†å²æ–‡æœ¬
                        enhanced_history = history

            # ç¬¬å››æ­¥ï¼šé€šè¿‡ç”Ÿæˆæ¨¡å‹è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆå¯è®­ç»ƒï¼‰
            if mode == "train":
                # è®­ç»ƒæ¨¡å¼ï¼šä½¿ç”¨findingsè®¡ç®—æŸå¤±
                outputs = self.findings_decoder(
                    visual_features=enhanced_visual_features,  # ä½¿ç”¨Cross-Attentionå¢å¼ºåçš„è§†è§‰ç‰¹å¾
                    history_encoding=enhanced_history,  # ä½¿ç”¨å¢å¼ºåçš„å†å²æ–‡æœ¬
                    findings=findings,
                    use_history=True
                )
                
                return outputs
            else:
                # çº¯ç”Ÿæˆæ¨¡å¼ï¼šä¸è®¡ç®—æŸå¤±ï¼Œåªç”Ÿæˆæ–‡æœ¬
                with torch.no_grad():
                    generated_texts = self.findings_decoder.generate(
                        visual_features=enhanced_visual_features,  # ä½¿ç”¨Cross-Attentionå¢å¼ºåçš„è§†è§‰ç‰¹å¾
                        history_encoding=enhanced_history,  # ä½¿ç”¨å¢å¼ºåçš„å†å²æ–‡æœ¬
                        use_history=True
                    )
                return {"findings_text": generated_texts}

        elif phase == "BUILD_DATABASE":
            # BUILD_DATABASEé˜¶æ®µï¼šæå–è§£å‰–åŒºåŸŸç‰¹å¾ç”¨äºæ„å»ºæ•°æ®åº“
            with torch.no_grad():
                # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ç›®æ ‡æ£€æµ‹å™¨æå–åŒºåŸŸç‰¹å¾
                detection_outputs = self.object_detector(
                    image,
                    bbox_targets,
                    current_epoch=current_epoch,
                    total_epochs=total_epochs,
                )
                region_features = detection_outputs["region_features"]  # [B, 29, 768]
                region_detected = detection_outputs["region_detected"]  # [B, 29]

                # ç¬¬äºŒæ­¥ï¼šé€šè¿‡ViTå¤„ç†åŒºåŸŸç‰¹å¾
                image_encoder_outputs = self.image_encoder(
                    region_features, 
                    region_detected=region_detected, 
                    image_labels=label,
                    phase="PRETRAIN_VIT",  # ä½¿ç”¨PRETRAIN_VITæ¨¡å¼ï¼Œä¸å¯ç”¨MOE
                    use_moe=False
                )

                # è·å–å®Œæ•´çš„è§†è§‰ç‰¹å¾
                visual_features = image_encoder_outputs["visual_features"]  # [B, 1+num_regions, hidden_size]
                
                # æå–åŒºåŸŸç‰¹å¾ï¼ˆå»é™¤CLS tokenï¼‰
                region_visual_features = visual_features[:, 1:, :]  # [B, 29, 768]

                return {
                    "region_features": region_visual_features,  # ViTå¤„ç†åçš„åŒºåŸŸç‰¹å¾
                    "region_detected": region_detected,  # åŒºåŸŸæ£€æµ‹æ©ç 
                    "raw_region_features": region_features,  # æ£€æµ‹å™¨åŸå§‹åŒºåŸŸç‰¹å¾
                }

    def compute_global_ltc_loss(self, visual_cls, text_cls, negative_samples):
        """
        è®¡ç®—ä½¿ç”¨å…¨å±€è´Ÿæ ·æœ¬æ± çš„è¯­è¨€-è§†è§‰å¯¹æ¯”æŸå¤±(LTC)ï¼Œå®Œå…¨å¹¶è¡Œå¤„ç†

        å‚æ•°:
            visual_cls: æ˜ å°„åçš„è§†è§‰ç‰¹å¾ [B, hidden_size]
            text_cls: æ˜ å°„åçš„æ–‡æœ¬ç‰¹å¾ [B, hidden_size]
            negative_samples: è´Ÿæ ·æœ¬tensor [B, K, hidden_size]

        è¿”å›:
            å…¨å±€å¯¹æ¯”æŸå¤±å€¼
        """
        batch_size = visual_cls.size(0)
        temperature = 0.07  # æ¸©åº¦å‚æ•°
        device = visual_cls.device

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(visual_cls).any() or torch.isinf(visual_cls).any():
            print("âš ï¸  visual_clsåŒ…å«NaNæˆ–Infå€¼")
            visual_cls = torch.nan_to_num(visual_cls, nan=0.0, posinf=1.0, neginf=-1.0)
            
        if torch.isnan(text_cls).any() or torch.isinf(text_cls).any():
            print("âš ï¸  text_clsåŒ…å«NaNæˆ–Infå€¼")
            text_cls = torch.nan_to_num(text_cls, nan=0.0, posinf=1.0, neginf=-1.0)
            
        if torch.isnan(negative_samples).any() or torch.isinf(negative_samples).any():
            print("âš ï¸  negative_samplesåŒ…å«NaNæˆ–Infå€¼")
            negative_samples = torch.nan_to_num(negative_samples, nan=0.0, posinf=1.0, neginf=-1.0)

        # å½’ä¸€åŒ–ç‰¹å¾ï¼Œæ·»åŠ å°çš„epsiloné¿å…é™¤é›¶
        eps = 1e-8
        visual_cls = F.normalize(visual_cls, p=2, dim=1, eps=eps)  # [B, hidden_size]
        text_cls = F.normalize(text_cls, p=2, dim=1, eps=eps)  # [B, hidden_size]
        negative_samples = F.normalize(
            negative_samples, p=2, dim=2, eps=eps
        )  # [B, K, hidden_size]

        # è®¡ç®—æ­£æ ·æœ¬ç›¸ä¼¼åº¦ [B]
        pos_similarities = torch.sum(visual_cls * text_cls, dim=1)

        # æ‰¹é‡è®¡ç®—è´Ÿæ ·æœ¬ç›¸ä¼¼åº¦ [B, K]
        neg_similarities = torch.bmm(
            visual_cls.unsqueeze(1),  # [B, 1, hidden_size]
            negative_samples.transpose(1, 2),  # [B, hidden_size, K]
        ).squeeze(
            1
        )  # [B, K]

        # åˆå¹¶ç›¸ä¼¼åº¦å¹¶åº”ç”¨æ¸©åº¦ç¼©æ”¾ [B, K+1]
        all_similarities = (
            torch.cat([pos_similarities.unsqueeze(1), neg_similarities], dim=1)
            / temperature
        )
        
        # æ•°å€¼ç¨³å®šæ€§ï¼šé™åˆ¶ç›¸ä¼¼åº¦èŒƒå›´ä»¥é˜²æ­¢æº¢å‡º
        all_similarities = torch.clamp(all_similarities, min=-10.0, max=10.0)

        # ä½¿ç”¨äº¤å‰ç†µè®¡ç®—æŸå¤±
        labels = torch.zeros(batch_size, dtype=torch.long, device=device)
        loss = F.cross_entropy(all_similarities, labels)
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸  LTCæŸå¤±è®¡ç®—å‡ºç°NaN/Infï¼Œè¿”å›é›¶æŸå¤±")
            return torch.tensor(0.0, device=device, requires_grad=True)
            
        return loss

    def compute_batch_ltc_loss(self, visual_cls, text_cls):
        """
        åœ¨æ‰¹æ¬¡å†…è®¡ç®—è¯­è¨€-è§†è§‰å¯¹æ¯”æŸå¤±(LTC)

        å‚æ•°:
            visual_cls: æ˜ å°„åçš„è§†è§‰ç‰¹å¾ [B, hidden_size]
            text_cls: æ˜ å°„åçš„æ–‡æœ¬ç‰¹å¾ [B, hidden_size]

        è¿”å›:
            æ‰¹å†…å¯¹æ¯”æŸå¤±å€¼
        """
        batch_size = visual_cls.size(0)
        temperature = (
            self.config.TEMPERATURE if hasattr(self.config, "TEMPERATURE") else 0.07
        )  # æ·»åŠ é»˜è®¤å€¼

        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(visual_cls).any() or torch.isinf(visual_cls).any():
            print("âš ï¸  visual_clsåŒ…å«NaNæˆ–Infå€¼")
            visual_cls = torch.nan_to_num(visual_cls, nan=0.0, posinf=1.0, neginf=-1.0)
            
        if torch.isnan(text_cls).any() or torch.isinf(text_cls).any():
            print("âš ï¸  text_clsåŒ…å«NaNæˆ–Infå€¼")
            text_cls = torch.nan_to_num(text_cls, nan=0.0, posinf=1.0, neginf=-1.0)

        # å½’ä¸€åŒ–ç‰¹å¾ï¼Œæ·»åŠ å°çš„epsiloné¿å…é™¤é›¶
        eps = 1e-8
        visual_cls = F.normalize(visual_cls, p=2, dim=1, eps=eps)
        text_cls = F.normalize(text_cls, p=2, dim=1, eps=eps)

        # è®¡ç®—æ‰€æœ‰è§†è§‰-æ–‡æœ¬å¯¹çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        logits = torch.matmul(visual_cls, text_cls.t()) / temperature  # [B, B]
        
        # æ•°å€¼ç¨³å®šæ€§ï¼šé™åˆ¶logitsèŒƒå›´
        logits = torch.clamp(logits, min=-10.0, max=10.0)

        # å¯¹è§’çº¿ä¸Šçš„å…ƒç´ æ˜¯æ­£æ ·æœ¬å¯¹
        labels = torch.arange(batch_size, device=visual_cls.device)

        # è®¡ç®—è§†è§‰->æ–‡æœ¬æ–¹å‘çš„æŸå¤±
        loss_v2t = F.cross_entropy(logits, labels)

        # è®¡ç®—æ–‡æœ¬->è§†è§‰æ–¹å‘çš„æŸå¤±
        loss_t2v = F.cross_entropy(logits.t(), labels)

        # æ€»æŸå¤±æ˜¯ä¸¤ä¸ªæ–¹å‘æŸå¤±çš„å¹³å‡
        loss = (loss_v2t + loss_t2v) / 2
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸  æ‰¹å†…LTCæŸå¤±è®¡ç®—å‡ºç°NaN/Infï¼Œè¿”å›é›¶æŸå¤±")
            return torch.tensor(0.0, device=visual_cls.device, requires_grad=True)

        return loss

    def compute_region_itc_loss(self, visual_features, region_detected, anatomical_embeddings_batch, image_ids=None):
        """
        è®¡ç®—åŒºåŸŸçº§åˆ«çš„å›¾åƒ-æ–‡æœ¬å¯¹æ¯”æŸå¤±(ITC) - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬
        
        å‚æ•°:
            visual_features: ViTè¾“å‡ºçš„è§†è§‰ç‰¹å¾ [B, 1+num_regions, hidden_size]
            region_detected: åŒºåŸŸæ£€æµ‹æ©ç  [B, num_regions]
            anatomical_embeddings_batch: æ‰¹æ¬¡ä¸­æ¯ä¸ªæ ·æœ¬çš„è§£å‰–åŒºåŸŸåµŒå…¥
            image_ids: å›¾åƒIDåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äºè°ƒè¯•ï¼‰
            
        è¿”å›:
            region_itc_loss: åŒºåŸŸçº§åˆ«çš„å¯¹æ¯”æŸå¤±ï¼Œå¦‚æœæ— æ³•è®¡ç®—åˆ™è¿”å›None
        """
        if not anatomical_embeddings_batch:
            return None
            
        batch_size = visual_features.size(0)
        device = visual_features.device
        
        # æœ€å¤§æ ·æœ¬æ•°é‡æ§åˆ¶
        max_samples = getattr(self.config, 'MAX_REGION_ITC_SAMPLES', 64)
        
        try:
            # é«˜æ•ˆæ•°æ®æ”¶é›†ï¼šé¿å…é‡å¤åˆ—è¡¨æ“ä½œ
            valid_pairs = []
            text_embeds_list = []
            
            # é¢„è®¡ç®—æ‰€æœ‰æ£€æµ‹maskï¼Œå‡å°‘GPUæŸ¥è¯¢æ¬¡æ•°
            detected_masks = region_detected > 0.5  # [B, 29]
            
            # æ”¶é›†æœ‰æ•ˆçš„è§†è§‰-æ–‡æœ¬å¯¹
            total_candidates = 0
            for batch_idx in range(batch_size):
                anatomical_embeddings = anatomical_embeddings_batch[batch_idx]
                if not anatomical_embeddings:
                    continue
                
                batch_mask = detected_masks[batch_idx]  # [29]
                
                for region_idx, text_embed in anatomical_embeddings.items():
                    if batch_mask[region_idx - 1]:  # 0-basedç´¢å¼•
                        total_candidates += 1
                        # æ—©æœŸéšæœºé‡‡æ ·æ§åˆ¶å†…å­˜
                        if total_candidates > max_samples:
                            if torch.rand(1).item() > (max_samples / total_candidates):
                                continue
                        
                        valid_pairs.append((batch_idx, region_idx - 1))
                        text_embeds_list.append(text_embed)
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            total_valid = len(valid_pairs)
            if total_valid < 2:
                return None
            
            # æœ€ç»ˆé‡‡æ ·ç¡®ä¿ä¸è¶…é™
            if total_valid > max_samples:
                indices = torch.randperm(total_valid)[:max_samples]
                valid_pairs = [valid_pairs[i] for i in indices]
                text_embeds_list = [text_embeds_list[i] for i in indices]
                total_valid = max_samples
            
            # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç‰¹å¾
            return self._compute_region_itc_direct(
                visual_features, valid_pairs, text_embeds_list, device
            )
                
        except Exception as e:
            print(f"âš ï¸  åŒºåŸŸITCæŸå¤±è®¡ç®—å‡ºé”™: {e}")
            return None

    def _compute_region_itc_direct(self, visual_features, valid_pairs, text_embeds_list, device):
        """ç›´æ¥è®¡ç®—åŒºåŸŸITCæŸå¤±ï¼Œå†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
        N = len(valid_pairs)
        
        # æ‰¹é‡æ„å»ºç´¢å¼•
        batch_indices = torch.tensor([pair[0] for pair in valid_pairs], device=device)
        region_indices = torch.tensor([pair[1] for pair in valid_pairs], device=device)
        
        # æå–åŒºåŸŸè§†è§‰ç‰¹å¾ - é¿å…é‡å¤åˆ‡ç‰‡
        region_visual = visual_features[:, 1:30, :]  # [B, 29, hidden_size]
        visual_feats = region_visual[batch_indices, region_indices]  # [N, hidden_size]
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(visual_feats).any() or torch.isinf(visual_feats).any():
            print("âš ï¸  åŒºåŸŸè§†è§‰ç‰¹å¾åŒ…å«NaNæˆ–Infå€¼")
            visual_feats = torch.nan_to_num(visual_feats, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # æ‰¹é‡è½¬æ¢æ–‡æœ¬ç‰¹å¾
        text_embeds = torch.stack(text_embeds_list).to(device, non_blocking=True)
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if torch.isnan(text_embeds).any() or torch.isinf(text_embeds).any():
            print("âš ï¸  åŒºåŸŸæ–‡æœ¬ç‰¹å¾åŒ…å«NaNæˆ–Infå€¼")
            text_embeds = torch.nan_to_num(text_embeds, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # æŠ•å½±å’Œå½’ä¸€åŒ– - åˆå¹¶æ“ä½œå‡å°‘å†…å­˜åˆ†é…
        eps = 1e-8
        mapped_visual = F.normalize(self.region_visual_projection(visual_feats), p=2, dim=1, eps=eps)
        mapped_text = F.normalize(self.region_text_projection(text_embeds), p=2, dim=1, eps=eps)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        temperature = getattr(self.config, 'REGION_ITC_TEMPERATURE', 0.07)
        logits = torch.matmul(mapped_visual, mapped_text.t()) / temperature  # [N, N]
        
        # æ•°å€¼ç¨³å®šæ€§ï¼šé™åˆ¶logitsèŒƒå›´
        logits = torch.clamp(logits, min=-10.0, max=10.0)
        
        # æ„å»ºæ ‡ç­¾
        labels = torch.arange(N, device=device, dtype=torch.long)
        
        # è®¡ç®—åŒå‘æŸå¤±
        loss_v2t = F.cross_entropy(logits, labels)
        loss_t2v = F.cross_entropy(logits.t(), labels)
        
        loss = (loss_v2t + loss_t2v) / 2
        
        # æœ€ç»ˆæ£€æŸ¥
        if torch.isnan(loss) or torch.isinf(loss):
            print("âš ï¸  åŒºåŸŸITCæŸå¤±è®¡ç®—å‡ºç°NaN/Infï¼Œè¿”å›é›¶æŸå¤±")
            return torch.tensor(0.0, device=device, requires_grad=True)
        
        return loss

    def apply_text_enhancement(self, visual_features, region_features, image_ids):
        """
        ä½¿ç”¨æ£€ç´¢åˆ°çš„æ–‡æœ¬ç‰¹å¾é€šè¿‡cross-attentionå¢å¼ºè§†è§‰ç‰¹å¾
        
        Args:
            visual_features: åŸå§‹è§†è§‰ç‰¹å¾ [B, 30, 768] (åŒ…å«CLS token)
            region_features: åŒºåŸŸè§†è§‰ç‰¹å¾ [B, 29, 768] (ä¸åŒ…å«CLS token)
            image_ids: å›¾åƒIDåˆ—è¡¨ï¼Œç”¨äºæ–‡æœ¬æ£€ç´¢
            
        Returns:
            enhanced_visual_features: å¢å¼ºåçš„è§†è§‰ç‰¹å¾ [B, 30, 768]
        """
        if not hasattr(self, 'text_to_visual_cross_attn') or self.text_enhancer is None:
            # å¦‚æœæ²¡æœ‰cross-attentionæ¨¡å—æˆ–æ–‡æœ¬å¢å¼ºå™¨ï¼Œç›´æ¥è¿”å›åŸå§‹ç‰¹å¾
            return visual_features
        
        try:
            # ä»é…ç½®ä¸­è·å–æ–‡æœ¬å¢å¼ºå‚æ•°
            similarity_threshold = getattr(self.config, 'TEXT_ENHANCEMENT_SIMILARITY_THRESHOLD', 0.5)
            top_k = getattr(self.config, 'TEXT_ENHANCEMENT_TOP_K', 1)
            top_sentences = getattr(self.config, 'TEXT_ENHANCEMENT_TOP_SENTENCES', 5)
            
            # æå–æ–‡æœ¬ç‰¹å¾è€Œä¸æ˜¯æ–‡æœ¬å­—ç¬¦ä¸² - æ¯ä¸ªåŒºåŸŸtop1
            text_features, valid_mask = self.text_enhancer(
                visual_features=region_features,
                query_image_ids=image_ids,
                similarity_threshold=similarity_threshold,
                top_k=top_k,
                return_features=True  # å…³é”®ï¼šè¿”å›ç‰¹å¾è€Œä¸æ˜¯æ–‡æœ¬
            )
            
            if text_features is None or not valid_mask.any():
                # å¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°æœ‰æ•ˆçš„æ–‡æœ¬ç‰¹å¾ï¼Œè¿”å›åŸå§‹è§†è§‰ç‰¹å¾
                return visual_features
            
            batch_size = visual_features.size(0)
            device = visual_features.device
            
            # æå–åŒºåŸŸç‰¹å¾ï¼ˆå»é™¤CLS tokenï¼‰è¿›è¡Œcross-attention
            region_visual_features = visual_features[:, 1:, :]  # [B, 29, 768]
            
            # æ£€æŸ¥å“ªäº›æ ·æœ¬æœ‰æœ‰æ•ˆçš„æ–‡æœ¬ç‰¹å¾
            sample_valid_mask = valid_mask.any(dim=1)  # [B]
            valid_indices = torch.where(sample_valid_mask)[0]
            
            if len(valid_indices) > 0:
                # æå–æœ‰æ•ˆæ ·æœ¬çš„ç‰¹å¾
                valid_region_visual = region_visual_features[valid_indices]  # [N_valid, 29, 768]
                valid_text = text_features[valid_indices]  # [N_valid, 29, 768]
                
                # Cross-Attention: æ–‡æœ¬ç‰¹å¾åšqueryï¼Œè§†è§‰ç‰¹å¾åškey&value
                enhanced_region_features, _ = self.text_to_visual_cross_attn(
                    query=valid_text,        # [N_valid, 29, 768]
                    key=valid_region_visual, # [N_valid, 29, 768]
                    value=valid_region_visual, # [N_valid, 29, 768]
                    need_weights=False
                )  # enhanced_region_features: [N_valid, 29, 768]
                
                # å›ºå®šæƒé‡èåˆï¼šalpha * enhanced + (1-alpha) * original
                alpha = getattr(self.config, 'TEXT_ENHANCEMENT_FUSION_WEIGHT', 0.3)  # å›ºå®šæƒé‡
                fused_region_features = alpha * enhanced_region_features + (1 - alpha) * valid_region_visual
                
                # å°†å¢å¼ºåçš„åŒºåŸŸç‰¹å¾æ”¾å›åŸå§‹tensorä¸­
                enhanced_visual_features = visual_features.clone()
                enhanced_visual_features[valid_indices, 1:, :] = fused_region_features
            else:
                enhanced_visual_features = visual_features
            
            return enhanced_visual_features
            
        except Exception as e:
            print(f"âŒ Cross-Attentionæ–‡æœ¬å¢å¼ºè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            # å‡ºé”™æ—¶è¿”å›åŸå§‹è§†è§‰ç‰¹å¾
            return visual_features
