# --- Base packages ---
import os
import json
import pickle
import re
import numpy as np
import pandas as pd

# --- PyTorch packages ---
import torch
import torch.utils.data as data
import torchvision.transforms as transforms

# --- Helper packages ---
from random import shuffle
import sentencepiece as spm
from PIL import Image, ImageFile

ImageFile.LOAD_TRUNCATED_IMAGES = True
from transformers import AutoTokenizer, AutoModel
from tqdm import tqdm
from collections import defaultdict
from utils import *


# --- Datasets ---
class MIMIC(data.Dataset):  # MIMIC-CXR Dataset
    # ç±»å˜é‡ç”¨äºå­˜å‚¨å…±äº«æ•°æ®
    _shared_data = {
        "loaded": False,
        "annotation": None,
        "anatomical_embeddings": None,  # æ–°å¢ï¼šå­˜å‚¨è§£å‰–åŒºåŸŸåµŒå…¥æ•°æ®
    }

    # 29ä¸ªè§£å‰–åŒºåŸŸçš„æ ‡å‡†åç§°æ˜ å°„ï¼ˆæŒ‰ç…§æ£€æµ‹å™¨è¾“å‡ºçš„é¡ºåºï¼Œä»1å¼€å§‹ï¼‰
    ANATOMICAL_REGIONS = [
        'left hemidiaphragm',      # 1
        'right atrium',            # 2  
        'right hilar structures',  # 3
        'cardiac silhouette',      # 4
        'abdomen',                 # 5
        'trachea',                 # 6
        'right apical zone',       # 7
        'right lung',              # 8
        'right upper lung zone',   # 9
        'right costophrenic angle', # 10
        'svc',                     # 11
        'left lung',               # 12
        'right mid lung zone',     # 13
        'cavoatrial junction',     # 14
        'left costophrenic angle', # 15
        'left hilar structures',   # 16
        'mediastinum',             # 17
        'right lower lung zone',   # 18
        'left mid lung zone',      # 19
        'spine',                   # 20
        'left upper lung zone',    # 21
        'right hemidiaphragm',     # 22
        'left clavicle',           # 23
        'aortic arch',             # 24
        'right clavicle',          # 25
        'left apical zone',        # 26
        'left lower lung zone',    # 27
        'carina',                  # 28
        'upper mediastinum'        # 29
    ]

    @classmethod
    def load_anatomical_embeddings(cls, anatomical_db_path):
        """
        åŠ è½½è§£å‰–åŒºåŸŸæ–‡æœ¬åµŒå…¥æ•°æ®
        
        å‚æ•°:
            anatomical_db_path: è§£å‰–åŒºåŸŸæ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        if cls._shared_data["anatomical_embeddings"] is not None:
            return  # å·²ç»åŠ è½½è¿‡äº†
            
        if anatomical_db_path is None or not os.path.exists(anatomical_db_path):
            print(f"âš ï¸  è§£å‰–åŒºåŸŸæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨æˆ–æœªé…ç½®: {anatomical_db_path}")
            cls._shared_data["anatomical_embeddings"] = {}
            return
            
        try:
            print(f"ğŸ“š æ­£åœ¨åŠ è½½è§£å‰–åŒºåŸŸæ•°æ®åº“: {anatomical_db_path}")
            with open(anatomical_db_path, 'rb') as f:
                data = pickle.load(f)
            
            if 'image_region_embeddings' in data:
                raw_embeddings = data['image_region_embeddings']
                metadata = data.get('metadata', {})
                print(f"âœ… æˆåŠŸåŠ è½½è§£å‰–åŒºåŸŸæ•°æ®åº“:")
                print(f"   - æ€»æ¡ç›®æ•°: {metadata.get('total_keys', len(raw_embeddings))}")
                print(f"   - å‘é‡ç»´åº¦: {metadata.get('embedding_dim', 'Unknown')}")
                print(f"   - æ¨¡å‹åç§°: {metadata.get('model_name', 'Unknown')}")
                
                # é‡æ–°ç»„ç»‡æ•°æ®ç»“æ„: image_id -> {region_index: tensor}
                organized_embeddings = defaultdict(dict)
                
                for key, embedding in tqdm(raw_embeddings.items(), desc="ç»„ç»‡è§£å‰–åŒºåŸŸæ•°æ®"):
                    try:
                        # è§£æé”®æ ¼å¼: image_id_region_index
                        parts = key.split('_')
                        if len(parts) >= 2:
                            region_index = int(parts[-1])  # æœ€åä¸€éƒ¨åˆ†æ˜¯åŒºåŸŸç´¢å¼•
                            image_id = parts[0]  # å‰é¢éƒ¨åˆ†æ˜¯image_id
                            
                            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)
                            organized_embeddings[image_id][region_index] = embedding_tensor  # ç›´æ¥èµ‹å€¼ï¼Œä¸append
                    except (ValueError, IndexError):
                        continue  # å¿½ç•¥æ ¼å¼ä¸æ­£ç¡®çš„é”®
                
                cls._shared_data["anatomical_embeddings"] = dict(organized_embeddings)
                print(f"ğŸ“Š ç»„ç»‡æ•°æ®å®Œæˆï¼Œè¦†ç›– {len(organized_embeddings)} ä¸ªå›¾åƒ")
                
            else:
                print(f"âŒ æ•°æ®åº“æ ¼å¼ä¸æ­£ç¡®ï¼Œç¼ºå°‘ 'image_region_embeddings' å­—æ®µ")
                cls._shared_data["anatomical_embeddings"] = {}
                
        except Exception as e:
            print(f"âŒ åŠ è½½è§£å‰–åŒºåŸŸæ•°æ®åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            cls._shared_data["anatomical_embeddings"] = {}

    @classmethod
    def load_shared_data(cls, directory, ann_dir, mode, extra_ann_dir=None, binary_mode=True):
        """é¢„å¤„ç†ä¼˜åŒ–ç‰ˆæœ¬ï¼ŒåŠ è½½MIMICæ•°æ®é›†æ³¨é‡Š"""
        if cls._shared_data["loaded"]:
            return

        # ä½¿ç”¨å†…å­˜æ˜ å°„è¯»å–å¤§å‹JSONæ–‡ä»¶
        import mmap

        with open(ann_dir, "r") as f:
            # å¯¹å¤§æ–‡ä»¶ä½¿ç”¨å†…å­˜æ˜ å°„
            mm = mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ)
            annotation_data = json.loads(mm.read().decode("utf-8"))
            mm.close()

        # å¹¶è¡Œå¤„ç†æ•°æ®æ‹†åˆ†
        import concurrent.futures

        new_annotation = {}

        def process_split(mode_split):
            mode, data_split = mode_split
            result = []
            for key, value in data_split.items():
                if value["findings"].strip() != "":
                    value["image_id"] = key
                    # é¢„å¤„ç†æ–‡æœ¬ï¼Œå‡å°‘__getitem__ä¸­çš„å¤„ç†æ—¶é—´
                    value["findings"] = cls._clean_report(value["findings"])
                    value["history"] = cls._clean_report(value["history"])
                    result.append(value)
            return mode, result

        # å¹¶è¡Œå¤„ç†å„ä¸ªæ‹†åˆ†
        with concurrent.futures.ThreadPoolExecutor() as executor:
            for mode, result in executor.map(process_split, annotation_data.items()):
                new_annotation[mode] = result

        if extra_ann_dir:
            with open(extra_ann_dir, 'r') as f:
                extra_ann = json.load(f)
            
            for id, item in extra_ann.items():
                item['image_id'] = id
                item['findings'] = cls._clean_report(item['findings'])
                item['history'] = cls._clean_report(item['history'])
                new_annotation['train'].append(item)
                
        cls._shared_data["annotation"] = new_annotation
        cls._shared_data["loaded"] = True



    def __init__(
        self,
        directory,
        ann_dir,
        extra_ann_dir=None,
        input_size=(224, 224),
        random_transform=True,
        tokenizer=None,
        mode="train",
        subset_size=None,
    ):

        self.load_shared_data(directory, ann_dir, mode, extra_ann_dir)

        self.tokenizer = tokenizer
        self.bos_token_id = self.tokenizer.bos_token_id
        self.eos_token_id = self.tokenizer.sep_token_id  # BERTä½¿ç”¨[SEP]ä½œä¸ºEOS
        self.pad_token_id = self.tokenizer.pad_token_id

        self.sources = ["image", "findings", "history", "bbox_targets"]
        self.targets = ["findings", "label"]

        self.dir = directory
        self.input_size = input_size
        self.random_transform = random_transform
        self.mode = mode
        self.subset_size = subset_size
        # ä½¿ç”¨å…±äº«æ•°æ®
        self.data = self._shared_data["annotation"][self.mode]

        if random_transform:
            self.transform = transforms.Compose(
                [
                    # transforms.Resize(224),
                    # transforms.RandomCrop(input_size),
                    # transforms.RandomRotation(degrees=5),
                    transforms.ToTensor(),
                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
                ]
            )
        else:
            self.transform = transforms.Compose(
                [
                    # transforms.Resize(224),
                    # transforms.CenterCrop(input_size),
                    transforms.ToTensor(),
                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
                ]
            )

    def __len__(self):
        if self.subset_size is not None:
            return self.subset_size
        else:
            return len(self.data)

    def __getitem__(self, idx):
        info = self.data[idx]

        findings = info["findings"]
        history = info["history"]
        disease_label = np.array(info["labels"], dtype=np.float16)
        image_id = info['image_id']
        
        # è·å–å›¾åƒè·¯å¾„
        image_base_path = "/".join(info["image_path"][0].split("/")[:-1])
        img_path = os.path.join(
            self.dir, "images_224", image_base_path, info["image_id"] + ".jpg"
        )

        # å¤„ç†å›¾åƒ
        img = Image.open(img_path).convert("RGB")
        img = self.transform(img)

        # å¤„ç†bbox
        if "bbox_targets" in info:
            bbox_data = info["bbox_targets"]
            boxes = torch.tensor(bbox_data["boxes"], dtype=torch.float32)
            labels = torch.tensor(bbox_data["labels"], dtype=torch.int64)  # æ ‡ç­¾å·²ç»æ˜¯ä»1å¼€å§‹çš„

            # éªŒè¯å¹¶è¿‡æ»¤è¾¹ç•Œæ¡†
            valid_boxes = []
            valid_labels = []
            for box, label in zip(boxes, labels):
                # æ£€æŸ¥è¾¹ç•Œæ¡†çš„å®½åº¦å’Œé«˜åº¦æ˜¯å¦å¤§äº0
                width = box[2] - box[0]
                height = box[3] - box[1]
                if width > 0 and height > 0:
                    valid_boxes.append(box)
                    valid_labels.append(label)

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„è¾¹ç•Œæ¡†ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„ç›®æ ‡
            if not valid_boxes:
                boxes = torch.zeros((0, 4), dtype=torch.float32)
                labels = torch.zeros((0,), dtype=torch.int64)
            else:
                boxes = torch.stack(valid_boxes)
                labels = torch.stack(valid_labels)
        else:
            boxes = torch.zeros((0, 4), dtype=torch.float32)
            labels = torch.zeros((0,), dtype=torch.int64)

        target = {
            "boxes": boxes,  # [N, 4] tensorï¼Œæ ¼å¼ä¸º (x1, y1, x2, y2)
            "labels": labels,  # [N] tensorï¼Œå·²ç»æ˜¯ä»1å¼€å§‹çš„ç±»åˆ«æ ‡ç­¾
            "image_id": torch.tensor([idx]),
            "area": (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1]),
            "iscrowd": torch.zeros((len(boxes),), dtype=torch.int64),
        }

        # è·å–è¯¥å›¾åƒçš„è§£å‰–åŒºåŸŸæ–‡æœ¬åµŒå…¥ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        anatomical_embeddings = {}
        if self._shared_data["anatomical_embeddings"] and image_id in self._shared_data["anatomical_embeddings"]:
            anatomical_embeddings = self._shared_data["anatomical_embeddings"][image_id]

        output = {
            "image": img,
            "image_id": image_id,
            "bbox_targets": target,
            "findings": findings,
            "history": history,
            "label": disease_label,
            "image_path": img_path,
            "anatomical_embeddings": anatomical_embeddings,  # æ–°å¢ï¼šè¯¥å›¾åƒçš„è§£å‰–åŒºåŸŸåµŒå…¥
        }

        return output

    @staticmethod
    def _clean_report(report):
        report_cleaner = (
            lambda t: t.replace("\n", " ")
            .replace("__", "_")
            .replace("__", "_")
            .replace("__", "_")
            .replace("__", "_")
            .replace("__", "_")
            .replace("__", "_")
            .replace("__", "_")
            .replace("  ", " ")
            .replace("  ", " ")
            .replace("  ", " ")
            .replace("  ", " ")
            .replace("  ", " ")
            .replace("  ", " ")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("..", ".")
            .replace("1. ", "")
            .replace(". 2. ", ". ")
            .replace(". 3. ", ". ")
            .replace(". 4. ", ". ")
            .replace(". 5. ", ". ")
            .replace(" 2. ", ". ")
            .replace(" 3. ", ". ")
            .replace(" 4. ", ". ")
            .replace(" 5. ", ". ")
            .strip()
            .lower()
            .split(". ")
        )
        sent_cleaner = lambda t: re.sub(
            "[.,?;*!%^&_+():-\[\]{}]",
            "",
            t.replace('"', "")
            .replace("/", "")
            .replace("\\", "")
            .replace("'", "")
            .strip()
            .lower(),
        )
        tokens = [
            sent_cleaner(sent)
            for sent in report_cleaner(report)
            if sent_cleaner(sent) != []
        ]
        report = " . ".join(tokens) + " ."
        return report


# æ·»åŠ collate_fnå‡½æ•°å¤„ç†å˜é•¿æ•°æ®
def mimic_collate_fn(batch):
    """ä¼˜åŒ–çš„collate_fn"""
    # ä½¿ç”¨é¢„åˆ†é…å†…å­˜è€ŒéåŠ¨æ€å¢é•¿çš„åˆ—è¡¨
    batch_size = len(batch)

    # é¢„åˆ†é…NumPyæ•°ç»„
    images = torch.empty((batch_size, 3, 224, 224), dtype=torch.float32)
    bbox_targets = [None] * batch_size
    findings = [None] * batch_size
    histories = [None] * batch_size
    labels = np.empty((batch_size, 14), dtype=np.float16)  # å‡è®¾æœ‰14ä¸ªæ ‡ç­¾
    image_paths = [None] * batch_size
    image_ids = [None] * batch_size
    anatomical_embeddings = [None] * batch_size  # æ–°å¢ï¼šè§£å‰–åŒºåŸŸåµŒå…¥

    # å¡«å……é¢„åˆ†é…çš„æ•°ç»„
    for i, item in enumerate(batch):
        images[i] = item["image"]
        bbox_targets[i] = item["bbox_targets"]
        findings[i] = item["findings"]
        histories[i] = item["history"]
        labels[i] = item["label"]
        image_paths[i] = item["image_path"]
        image_ids[i] = item["image_id"]
        anatomical_embeddings[i] = item["anatomical_embeddings"]  # æ–°å¢

    # è½¬æ¢æ ‡ç­¾
    label_tensor = torch.from_numpy(labels)

    return {
        "image": images,
        "bbox_targets": bbox_targets,
        "findings": findings,
        "history": histories,
        "label": label_tensor,
        "image_path": image_paths,
        "image_id": image_ids,
        "anatomical_embeddings": anatomical_embeddings,  # æ–°å¢ï¼šè§£å‰–åŒºåŸŸåµŒå…¥
        "gts": (findings, [""]*len(findings)),  # æ·»åŠ gtså­—æ®µä¿æŒå…¼å®¹æ€§
        "split": ["train"]*len(findings),  # æ·»åŠ splitå­—æ®µä¿æŒå…¼å®¹æ€§
    }
